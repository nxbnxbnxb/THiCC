
  Remember to praise his past work!    Especially "Expressive Body Capture: 3D Hands, Face, and Body from a Single Image;" this is a recent development and he probably put many hours in / funded it heavily.  I'm sure they're still working on this


  Max Planck Institute is the most-Nobeled institute in the world.  The blood of great scientists, philosophers, and innovators coarses through the academic veins of each student and teacher in the institute, and every stone is endowed with history and dignity.  I submit my application for a Ph.D to pursue these ideals of science and scholarship with one of your esteemed colleagues, Michael J. Black.  Technology propels man forward; without it, we would all still be huddled in huts on the Kalahari, vulnerable to malaria, in constant fear of lions and rival tribes, and without clean water or regular food.
  As for the topic of my Ph.D studies, I am interested in accurate-to-the-inch shape recovery.  Such a technology could usher in a new era of commerce for the world, with MPII at the helm, where it deserves to be.  I call is "HSR," or "Human Shape Recovery," in line with IMPRS' Dr. Angjoo Kanazawa's celebrated work "End-to-end Recovery of Human Shape and Pose," more colloquially known simply as "HMR."  HSR requires only 3 changes, all of which I am qualified to execute.  First, a segmentation that finds the silhouette of a person within inches of accuracy.  I have the code for such a segmentation on my laptop right now.  Second, a modified loss function that calculates reprojection loss of the SMPL mesh compared to that silhouette (every term in the SMPLify-X energy function is correct except for this reprojection loss in the data term; the state of the art, SMPLify-X, uses joint reprojection, which does not fully reflect the human form, unlike the silhouette) [TODO: find a suitable function in the literature with a grandiose name to insert here].  Third and lastly, the projection from 3-D to 2-D using the camera parameters K and perspective projection.
  For HSR's shape-parameter-beta-fitting, Google's deeplab segmentation is worse than the 2-image "diff" segmentation I wrote (TODO: see if the literature has a fancy name for what you "invented").  This is because deeplab cuts the stomach too wide, whereas "NXB diff" segmentation gives a very tight cut around the chest, stomach, and hips.  As you can see here: [link to diff segmentation], the only ways my "NXB diff" segmentation performs worse than deeplab is 1. the salt-and-pepper noise outside the body and 2. sometimes my "NXB diff" cuts out internal real parts of the body due to lighting conditions.  1. The salt-and-pepper noise is unfortunate, but largely irrelevant for reprojection loss because the noise is not often concentrated in large numbers of pixels near the body; instead it is randomly dispersed and therefore won't negatively affect the reprojection loss.  2. this minor shortcoming can be mitigated by either A. letting HSR learn the hyperparameter "delta" end-to-end as Kanazawa et al. demonstrate in HMR (2018) or B. deliberately and manually learning this hyperparameter as seen in Expressive ... ("SMPLify-X"), Pavlakos et al. (2019).
  The technical underpinning of the thesis I'm advancing is that it is the reprojection loss of the (begin italicize) **SILHOUETTE** (end italicize) that should be the data term for the objective function in "Expressive Body Capture: 3D Hands, Face, and Body from a Single Image."  Everything else is beautifully done: the integration of hands and face, the Variational Human Body Pose Prior [TODO: double-triple-check this by rereading "Expressive Body Capture: 3D Hands, Face, and Body from a Single Image" SMPL-X], the Collision penalizer to prevent the SMPL model's self-penetration, the E_[alpha] penalty limiting elbow and knee joint movement, the camera parameters K, etc.
  The weak perspective model and/or orthographic projection put forth in HMR is a viable assumption only when the depth of the individual is negligible compared to the distance between the camera and the target individual.  While this may be true in the in-the-wild images one can find in MS COCO or on the internet, for a commercial application wherein a tiny camera is placed on the ground and the target person has deliberately posed such that one arm is facing the camera and the other is pointing away, this assumption breaks down very quickly.  
  To usher in a new age of 3-D clothing e-commerce, and bring innovative science and technology to the world in the name of IMPRS and Max Planck Institute, I humbly ask for the opportunity to implement accurate-to-the-inch shape-recovery.  And this must happen from plain RGB images, so we need the "NXB diff" [appeal to the ETHOS of some famous statistician/programmer like the "Mahanolobis distance" mentioned in "Expressive Body Capture: 3D Hands, Face, and Body from a Single Image" SMPLify-X] described herein.  I will take care of the details, but this project will advance the human race and give credit where it is due, to the Max Planck Institution for Intelligent Systems and [TODO: insert the full name of IMPRS].  Centuries from now, people will still tell tales of our adventures in science, our vision, and the innovations that came from the institution of Max Planck Institute, just as we now speak of Albert Einstein's time at Princeton's Institute for Advanced Study, Alexander the Great in Constantinople, and The Wright Brothers at Kitty Hawk.





  TODO:
    Fill in "TODO"s wherever they appear.






























































