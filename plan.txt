________________________________

  Tue Mar  5 08:26:44 EST 2019
________________________________

  This plan WILL change, but at the same time it's important to organize ideas.

  Basically, the idea is to:
  1.  Force data acquisition in Jesus pose (or really any controlled pose)
  2.  frames  = cut(vid)
  3.  mask    = seg(frame)
  4.  angle   = detect(mask, vid) 
    #Autodetect angle of frame
    This part didn't work.
  5.  model   = upd8(model, mask, angle)
    #Use that angle and the image (and resultant segmentation mask) at that angle to mask the voxels
  6.  smpl    = fit(model)
    #Based on some loss function, fit SMPL model to data from customer video.
    #Potential metrics:
    a.  L2 distance between shell and SMPL mesh pts
    b.  Perimeter of a convex hull surrounding the points at that height
    c.  Area of slice @ that height vs area of a slice of the SMPL model @ that height



















































































  Basically, the idea is to:
  1.  Force data acquisition in Jesus pose (or really any controlled pose)
  2.  frames = cut(vid)
    a.  Color images, variable time between frames
  3.  mask  = seg(frame)
  4.  Autodetect angle of frame
    a.  Front frame: 0th frame ought to be frontal (ie. Jesus)
      1.  All front/back frames have long wingspan
        a. ie. 0 degrees, 180 degrees
      2.
# nOTE: in order to get the masks at the ideal angles of the body's rotation, we gotta come up with some smart way of calculating the angles.  Maybe counting the total number of img files between 0 and 360 degrees and just dividing?  It'll probably do for now, but unfortunately stepping in a circle is not like a smooth lazy-susan
      3.
    b.  Side frame: after 0th frame, minimize x-spread of mask (something like   "frame_idx=np.argmin(np.max(x_spreads))")
      1.  All side frames have "small wingspan"
        a. ie. 90 degrees, 270 degrees
      2.
      3.
      4.
    c.
      1. What if we went "backwards" instead?  Given an angle, we can find masks that are approx at that angle.
      2.  Can also autocut the video around first and last "moving frames" (ie. milliseconds before 1st moving frame and milliseconds after last moving frame)
        a.  Detect motion by a "diff" between frames, or between frames that are 10 frames apart
        b.  This "autocut" is not a key feature, though.  We can manually cut vids and/or help teach the user to take the video in a particular way.
    d.
    e.
  5.  Use that angle and the image (and resultant segmentation mask) at that angle to mask the voxels
    a.  The voxel shell gives us something easy to visualize & debug.
      1.  Marching Cubes ====> shitty mesh
      2.
    b.
    c.
  6.  Based on some metric (loss function), fit the SMPL model to the shell (skin voxel data).  Potential metrics:
    a.  L2 distance between shell and SMPL mesh pts
      1.  Try closest 2 points (ie. kdtree)
      2.  Distance along a normal vector between the point and the SMPL model?  But that assumes we can somehow get the normal in the 1st place...
    b.  Perimeter of a convex hull surrounding the points at that height
    c.  Area of slice @ that height vs area of a slice of the SMPL model @ that height
    d.

    e.  NOTE:  Use coordinate/gradient descent/ascent on only a fewwww betas (ie. 0th beta, 1th beta, 2th beta, etc.)
    f.  Regularization is important.  We want those betas SMALL.
    g.
    h.  But how do we differentiate the loss function w.r.t a particular beta?
      1.  I know how I'd set this up with coordinate ascent, but idk how to take a derivative.  Maybe with the definition of a derivative (ie. limit of best-fitting line through a few points; plz picture a few noisy data points with a best-fit line through them.   Beta is on the x-axis and L2 error btwn SMPL and the voxels is on the y-axis.) then take the slope and that's your derivative
    i.
  7.  Since chumpy does lots of the work itself, maybe we don't need tensorflow?
    a.  Lookup whether tensorflow can do coordinate descent
    b.  Also TODO: look @ HMR for example tensorflow code
    c.
    d.
    e.
    f.
  8.
  9.
  10.
  11.





  How to do 1 mask => smpl?

  0.  Scale mask / smpl to the same size
  1.  Apply perspective adjustment to mask
    a.  Orthographic vs cartesian vs. whatever
  2.  Shift mask and mesh to same position 
  3.  Penalize places where 
    A) The mask is and the mesh isn't and 
    B) The mesh is and mask isn't
    C) Regularization (betas should be small)
  4.  F
  5.
  6.
  7.
  8.
  9.
  10.
  11.
  12.
  13.
  14.
  15.
  16.
  17.
  18.



































































  Get data from online
    3 days for males
    Try scrapy?
    Probably we want all standing pose
  Fit first 2 betas based on this data (give scrapy a shot too)
    Pitch it
  1 week for females

  What does Dario actually NEED to fix the visa issues?
    When does this need to be done BY?
      Walk backwards from this and be aggressive (get a worst-case scenario)

  1.
    a.
  2.
  3.
  4.
  5.
  6.
  7.
  8.
  9.
  10.
  11.
  12.
  13.
  14.
  15.
  16.
  17.
  18.







































































  For "VHMR" (360-degree-Video-HMR), I think the steps should be (all modular as possible:
    Cut up
    Get best-estimated SMPL object out of the single image
    Store all the betas in one data structure and take the median, mean, [insert some "sensibly calculated" middle value](really just means whatever betas minimize L2 error (haha, as if it were possible to know this without access to the CAESAR dataset)), etc.


















































































Rudimentary web server (for integration with Pierlorenzo's part)
  src/web/flask_docs_test/upload_example_docs/1/img_to_mesh.py
LOCATION OF OLD CODE YOU MIGHT WANT LATER



























































